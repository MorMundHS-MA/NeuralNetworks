---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import math
```

```{python}
class SLN:
    def __init__(self, dIn, cOut):
        np.random.seed(42)
        self._W = np.random.randn(cOut, dIn) / math.sqrt(dIn + 1)
        self._b = np.zeros((cOut, 1))
    def neuron(self, X):
        net = np.dot(self._W, X) + self._b
        return net > 0
    def neuronWithFor(self, X):
        net = np.zeros(X.shape[1])
        for i in range(0, X.shape[1]):
            net[i] = (X[:,i] * self._W[0]).sum() + self._b[0]
        return net > 0
    def errorRate(Y,T):
        if Y.ndim==1 or Y.shape[0]==1:
            errors=Y!=T
            return errors.sum()/Y.size
        else: # fÃ¼r mehrere Ausgaben in one-hot Kodierung:
            errors=Y.argmax(0)!=T.argmax(0)
            return errors.sum()/Y.shape[1]            
```

```{python}
def origNeuron(X, W = [-0.3,1], threshold = 2):
    if W == None:
        W = [-0.3,1]
    if threshold == None:
        threshold = 2
    net = np.zeros(X.shape[1])
    for i in range(0, X.shape[1]):
        net[i] = (X[:,i] * W).sum()
    return net > threshold
```

```{python}
data = np.loadtxt(fname='data/iris.csv', delimiter=',')
X = data[:,0:4].T
T = data[:,4].T
```

```{python}
exercise1 = origNeuron(X[:2,:])
ex1new = SLN(2,1)
ex1new._W = np.array([[-0.3, 1]])
ex1new._b = np.array([[-2]])
```

```{python}
np.array_equal(ex1new.neuron(X[:2,:])[0], exercise1)
```

```{python}
SLN.errorRate(ex1new.neuronWithFor(X[:2,:]),T = 0)
```

```{python}
np.array_equal(ex1new.neuron(X[:2, :])[0], ex1new.neuronWithFor(X[:2, :]))
```

```{python}
import depends.nnwplot as nnwplot

class SLN(SLN):
    def deltaTrain(self, X, T, eta, maxIter, maxErrorRate):
        i = 0
        errorRate = 1
        bestError = 1
        bestW = self._W
        bestb = self._b
        plt.ion()
        y = self.neuron(X).astype(int)
        while i < maxIter and errorRate >= maxErrorRate:
            i += 1
            self._W += (eta * np.dot((T - y), X.T)) / T.shape[0]
            self._b += (eta * np.dot((T - y), np.ones(T.shape[0]))) / T.shape[0]
            y = self.neuron(X).astype(int)
            errorRate = SLN.errorRate(self.neuron(X).astype(int), T)
            if i % (maxIter / 10) == 0:
                print(f"Iteration {i} of {maxIter}. Error: {errorRate*100:{10}.{2}}%")
                nnwplot.plotTwoFeaturesA(X[0:2,:],T , self.neuron)
            if errorRate < bestError:
                bestError = errorRate
                bestW = self._W.copy()
                bestb = self._b.copy()
        
        self._W = bestW
        self._b = bestb
        print(f"Best Error: {bestError*100:{10}.{2}}%")
        nnwplot.plotTwoFeaturesA(X[0:2,:],T , self.neuron)

```

### Training for the Iris Setosa (Type 0 in Data)

```{python}
delta = SLN(2, 1)
delta.deltaTrain(X[:2, :], (T == 0).astype(int), 0.05, 40, 0.01)

```

### Training an AND Gate

```{python}
andGate = SLN(2, 1)
andIn = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])
andOut = andIn[0] & andIn[1]
print(andIn)
print(andOut)
```

```{python}
andGate.deltaTrain(andIn, andOut, 0.5, 10, 0.01)
print(andGate._W)
print(andGate._b)
```

### Training for the Iris Versicolour (Type 1 in Data)

```{python}
versi = SLN(2, 1)
versi.deltaTrain(X[:2, :], (T == 1).astype(int), 0.05, 200, 0.01)
```

### Versicolour using the second and third feature

```{python}
versi = SLN(2, 1)
versi.deltaTrain(X[2:4, :], (T == 1).astype(int), 0.05, 200, 0.01)
```

### AND gate using Hebb training

```{python}
class SLN(SLN):
    def hebbTrain(self, X, T, eta, maxIter, maxErrorRate):
        i = 0
        errorRate = 1
        bestError = 1
        bestW = self._W
        bestb = self._b
        plt.ion()
        y = self.neuron(X).astype(int)
        while i < maxIter and errorRate >= maxErrorRate:
            i += 1
            self._W += (eta * np.dot(T, X.T)) / T.shape[0]
            self._b += (eta * np.dot(T, np.ones(T.shape[0]))) / T.shape[0]
            y = self.neuron(X).astype(int)
            errorRate = SLN.errorRate(self.neuron(X).astype(int), T)
            if i % (maxIter / 10) == 0:
                print(f"Iteration {i} of {maxIter}. Error: {errorRate*100:{10}.{2}}%")
                nnwplot.plotTwoFeaturesA(X[0:2,:],T , self.neuron)
            if errorRate < bestError:
                bestError = errorRate
                bestW = self._W.copy()
                bestb = self._b.copy()
        
        self._W = bestW
        self._b = bestb
        print(f"Best Error: {bestError*100:{10}.{2}}%")
        nnwplot.plotTwoFeaturesA(X[0:2,:],T , self.neuron)
hebbAND = SLN(2, 1)
hebbAND.hebbTrain(andIn, andOut, 0.1, 10, 0.01)
```
